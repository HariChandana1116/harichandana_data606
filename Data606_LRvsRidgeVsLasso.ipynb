{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Data606-LRvsRidgeVsLasso.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 21,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iOoE3XgPu8wB",
        "outputId": "4756ffd2-cb49-4b88-bb08-dbbc6b81b9a2"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Drive already mounted at /content/gdrive; to attempt to forcibly remount, call drive.mount(\"/content/gdrive\", force_remount=True).\n"
          ]
        }
      ],
      "source": [
        "## google drive to google colab\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import pandas as pd"
      ],
      "metadata": {
        "id": "tt5ZcGD4vQew"
      },
      "execution_count": 22,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "df = pd.read_csv('/content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv')"
      ],
      "metadata": {
        "id": "idXLdfXlvTWQ"
      },
      "execution_count": 23,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Since there are many measures in the Measures coulumn after the EDA, we have goruped them into 2 different lists"
      ],
      "metadata": {
        "id": "blczgU_J5guM"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "prevention_cols = [\n",
        "    'ACCESS2', 'BPMED', 'CHECKUP', 'CHOLSCREEN',\n",
        "    'COLON_SCREEN', 'COREM', 'COREW', 'DENTAL', 'MAMMOUSE', 'PAPTEST']\n",
        "\n",
        "behavior_cols = ['BINGE', 'CSMOKING', 'LPA', 'OBESITY', 'SLEEP']\n",
        "\n",
        "outcome_cols = [\n",
        "    'ARTHRITIS', 'BPHIGH', 'CANCER',\n",
        "    'CASTHMA', 'CHD', 'COPD',\n",
        "    'DIABETES', 'HIGHCHOL', 'KIDNEY',\n",
        "    'MHLTH', 'PHLTH', 'STROKE', 'TEETHLOST']\n",
        "\n",
        "columns_to_drop = [ 'Year', 'StateAbbr', 'DataSource', 'Measure','Data_Value_Unit',\n",
        "                    'Data_Value_Footnote', 'Data_Value_Type', 'Low_Confidence_Limit',\n",
        "                    'High_Confidence_Limit', 'Data_Value_Footnote_Symbol',\n",
        "                    'CategoryID', 'Short_Question_Text']\n",
        "\n",
        "columns_to_keep = ['StateDesc', 'Category', 'CityName', 'UniqueID', 'GeographicLevel', 'DataValueTypeID',\n",
        "                   'PopulationCount', 'CityFIPS', 'TractFIPS', 'GeoLocation']\n",
        "\n",
        "random_state = 10"
      ],
      "metadata": {
        "id": "vzXHmy8EvdVi"
      },
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def get_data(path):\n",
        "    data = pd.read_csv(path)\n",
        "\n",
        "    # drop unecessary columns\n",
        "    data = data.drop(columns=columns_to_drop)\n",
        "    # print(data.columns)\n",
        "\n",
        "    # remove age-adjusted data\n",
        "    data = data.drop(data[data.DataValueTypeID == 'AgeAdjPrv'].index)\n",
        "\n",
        "    census_tract_data = data[data['GeographicLevel'] == 'Census Tract']\n",
        "    city_data = data[data['GeographicLevel'] == 'City']\n",
        "\n",
        "    tract_pv = census_tract_data.pivot_table(index=['StateDesc', 'CityName', 'UniqueID'], columns='MeasureId',\n",
        "                                             values='Data_Value',\n",
        "                                             aggfunc='sum')\n",
        "    print(\"Size of census tract data:\", len(tract_pv))  # 28004\n",
        "    tract_pv = tract_pv.fillna(tract_pv.mean())\n",
        "\n",
        "    city_pv = city_data.pivot_table(index=['StateDesc', 'CityName', 'UniqueID'], columns='MeasureId',\n",
        "                                    values='Data_Value', aggfunc='sum')\n",
        "    print(\"Size of city data:\", len(city_pv))  # 500\n",
        "    city_pv = city_pv.fillna(city_pv.mean())\n",
        "\n",
        "    city_pv.reset_index(level=0, inplace=True)\n",
        "    tract_pv.reset_index(level=0, inplace=True)\n",
        "\n",
        "    return data, city_pv, tract_pv"
      ],
      "metadata": {
        "id": "Z60O94_SvoEz"
      },
      "execution_count": 25,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import numpy as np\n",
        "from sklearn.feature_selection import RFE\n",
        "from sklearn.linear_model import LinearRegression, Lasso, Ridge\n",
        "from sklearn.metrics import r2_score\n",
        "from sklearn.model_selection import train_test_split, GridSearchCV\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn.svm import SVR"
      ],
      "metadata": {
        "id": "skJXMF7uvt3R"
      },
      "execution_count": 26,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def multi_model_analysis(clean_data):\n",
        "    # split data to training and testing set\n",
        "    x_data = clean_data.loc[:, clean_data.columns.isin(prevention_cols + behavior_cols)]\n",
        "    i = 0\n",
        "    for outcome_col in outcome_cols:\n",
        "        print(\"-----------------------------------\")\n",
        "        print(\"#\", i)\n",
        "\n",
        "        print(\"Health Outcome to analyze: \", outcome_col)\n",
        "        i += 1\n",
        "\n",
        "        y_data = clean_data.loc[:, outcome_col]\n",
        "        # split data to training and testing set\n",
        "        x_train, x_test, y_train, y_test = train_test_split(x_data, y_data, train_size=0.7,\n",
        "                                                            random_state=random_state)\n",
        "\n",
        "        # Linear Regression\n",
        "        reg = LinearRegression().fit(x_train, y_train)\n",
        "        # print(reg.intercept_)\n",
        "        print(\"lm coef:\", reg.coef_)\n",
        "        print(\"lm train score:\", round(reg.score(x_train, y_train), 4))\n",
        "        print(\"lm test score:\", round(reg.score(x_test, y_test), 4))\n",
        "\n",
        "        #     X = sm.add_constant(x_train)\n",
        "        #     lm = sm.OLS(y_train, X).fit()\n",
        "        #     print(lm.summary())\n",
        "\n",
        "        # Ridge Regression\n",
        "        reg = Ridge().fit(x_train, y_train)\n",
        "        print(reg.intercept_)\n",
        "        print(reg.coef_)\n",
        "        print(reg)\n",
        "        print(\"Ridge train score:\", reg.score(x_train, y_train))\n",
        "        print(\"Ridge test score:\", reg.score(x_test, y_test))\n",
        "\n",
        "        # Lasso Regression\n",
        "        reg = Lasso().fit(x_train, y_train)\n",
        "        print(\"Lasso coef:\", reg.coef_)\n",
        "        print(\"Lasso train score:\", round(reg.score(x_train, y_train), 4))\n",
        "        print(\"Lasso test score:\", round(reg.score(x_test, y_test), 4))\n",
        "\n",
        "        # SVM - regression with Hyper-parameter Tuning\n",
        "        svr = SVR()\n",
        "        parameters = {'kernel': ('linear', 'rbf'), 'C': [0.01, 1, 10]}\n",
        "        clf = GridSearchCV(svr, parameters, cv=10).fit(x_train, y_train)\n",
        "        best_param = clf.best_params_\n",
        "        print(\"svm best params:\", clf.best_params_)\n",
        "        print(\"svm best score:\", clf.best_score_)\n",
        "        print(\"svm best set train accuracy:\", round(r2_score(y_train, clf.predict(x_train)), 4))\n",
        "        print(\"svm best set test accuracy:\", round(r2_score(y_test, clf.predict(x_test)), 4))\n",
        "        # feature selection - recursive feature elimination\n",
        "        estimator = SVR(kernel=best_param['kernel'], C=best_param['C'])\n",
        "        selector = RFE(estimator, 5, step=1)  # top 5 features\n",
        "        selector = selector.fit(x_train, y_train)\n",
        "        # print(x_test.columns)\n",
        "        # print(selector.support_)\n",
        "        # print(selector.ranking_)\n",
        "        print(\"top features:\", x_test.columns[selector.support_])\n",
        "\n",
        "\n",
        "# Top n Feature selection on given data for the specified columns\n",
        "def feature_selection(clean_data, cols, n=5):\n",
        "    train_score = []\n",
        "    test_score = []\n",
        "    top_feature = []\n",
        "    for outcome_col in cols:\n",
        "        x_data = clean_data.loc[:, clean_data.columns.isin(prevention_cols + behavior_cols)]\n",
        "        y_data = clean_data.loc[:, outcome_col]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        x_scaled = scaler.fit_transform(x_data)\n",
        "\n",
        "        x_train_std, x_test_std, y_train, y_test = train_test_split(x_scaled, y_data, train_size=0.7,\n",
        "                                                                    random_state=random_state)\n",
        "\n",
        "        param_grid = {'kernel': ['linear'], 'C': [0.01, 1, 10]}\n",
        "        clf = GridSearchCV(SVR(), param_grid, cv=5)\n",
        "        clf = clf.fit(x_train_std, y_train)\n",
        "\n",
        "        train_scr = r2_score(y_train, clf.predict(x_train_std))\n",
        "        test_scr = r2_score(y_test, clf.predict(x_test_std))\n",
        "\n",
        "        train_score.append(train_scr)\n",
        "        test_score.append(test_scr)\n",
        "\n",
        "        selector = RFE(clf.best_estimator_, 1, step=1)\n",
        "        selector = selector.fit(x_train_std, y_train)\n",
        "        top_feature.append(sorted(zip(selector.ranking_, x_data.columns.tolist()), key=lambda x: x[0])[:n])\n",
        "    d = {'health_outcomes': cols, 'train_score': train_score, 'test_score': test_score, 'top_feature': top_feature}\n",
        "    return pd.DataFrame(data=d)\n",
        "\n",
        "\n",
        "def visual_data_prep(clean_data, prev_data, cols, n=5):\n",
        "    df = pd.DataFrame()\n",
        "    for outcome_col in cols:\n",
        "        print(\"analysing \", outcome_col)\n",
        "        x_data = clean_data.loc[:, clean_data.columns.isin(prevention_cols + behavior_cols)]\n",
        "        y_data = clean_data.loc[:, outcome_col]\n",
        "\n",
        "        scaler = StandardScaler()\n",
        "        x_scaled = scaler.fit_transform(x_data)\n",
        "\n",
        "        x_train_std, x_test_std, y_train, y_test = train_test_split(x_scaled, y_data, train_size=0.7,\n",
        "                                                                    random_state=random_state)\n",
        "\n",
        "        param_grid = {'kernel': ['linear'], 'C': [0.01, 1, 10]}\n",
        "        clf = GridSearchCV(SVR(), param_grid, cv=5)\n",
        "        clf = clf.fit(x_train_std, y_train)\n",
        "\n",
        "        train_scr = r2_score(y_train, clf.predict(x_train_std))\n",
        "        test_scr = r2_score(y_test, clf.predict(x_test_std))\n",
        "\n",
        "        selector = RFE(clf.best_estimator_, 1, step=1)\n",
        "        selector = selector.fit(x_train_std, y_train)\n",
        "        arr_tup = sorted(zip(selector.ranking_, x_data.columns.tolist()), key=lambda x: x[0])[:n]\n",
        "\n",
        "        prevention = [x[1] for x in arr_tup]\n",
        "        prevention_rank = [x[0] for x in arr_tup]\n",
        "        d = {'Prevention': prevention,'PreventionRank':prevention_rank}\n",
        "        temp = pd.DataFrame(data=d)\n",
        "\n",
        "        temp['Outcome'] = outcome_col\n",
        "        temp['PredictionAccuracyScore'] = train_scr\n",
        "\n",
        "        row = prev_data[prev_data.MeasureId == outcome_col]\n",
        "\n",
        "        temp['OutcomePrevalence'] = row.Data_Value.iloc[0]\n",
        "        temp['PreventionPrevalence'] = [prev_data[prev_data.MeasureId == p].Data_Value.iloc[0] for p in prevention]\n",
        "        df = pd.concat([temp, df])\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "def append_existing_data(df, prev_data):\n",
        "    temp = pd.DataFrame()\n",
        "    for outcome_col in outcome_cols:\n",
        "        outcome_row = prev_data[prev_data.MeasureId == outcome_col]\n",
        "\n",
        "        df2 = df[df['Outcome'] == outcome_col]\n",
        "        # Weighted average prevalence for the outcome\n",
        "        df2['OutcomePrevalence'] = np.sum(outcome_row.Data_Value * outcome_row.PopulationCount) / np.sum(\n",
        "            outcome_row.PopulationCount)\n",
        "        prevention = df2['Prevention'].iloc[:]\n",
        "\n",
        "        p_list = []\n",
        "        for p in prevention:\n",
        "            prevention_row = prev_data[prev_data.MeasureId == p]\n",
        "            # Weighted average prevalence for the prevention\n",
        "            prevention_prevalence = np.sum(\n",
        "                prevention_row.Data_Value * prevention_row.PopulationCount) / np.sum(\n",
        "                prevention_row.PopulationCount)\n",
        "            p_list.append(prevention_prevalence)\n",
        "\n",
        "        df2['PreventionPrevalence'] = p_list\n",
        "        temp = pd.concat([temp, df2])\n",
        "    return temp"
      ],
      "metadata": {
        "id": "x9Sd6432v4jn"
      },
      "execution_count": 38,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "import logging\n",
        "import sys\n",
        "import warnings"
      ],
      "metadata": {
        "id": "aBYaJ191v__F"
      },
      "execution_count": 39,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def initial_setup():\n",
        "    warnings.filterwarnings(\"ignore\")  # ignore warnings from sklean\n",
        "    global logger\n",
        "    logger = logging.getLogger()\n",
        "    logger.setLevel(logging.DEBUG)\n",
        "    ch = logging.StreamHandler()\n",
        "    logger.addHandler(ch)\n",
        "    logger.info(\"Logger ready\")\n",
        "\n",
        "\n",
        "def main():\n",
        "    initial_setup()\n",
        "    print(\"Accepted mode: \\n1. Perform top feature selection Analysis.\\n\"\n",
        "          \"2. Generate output file for visualization.\\n\"\n",
        "          \"3. Run multiple models comparision analysis.\\n\")\n",
        "    try:\n",
        "        mode = int(input('Enter the number of your selected mode:\\n'))\n",
        "    except ValueError:\n",
        "        print(\"Not a number\")\n",
        "\n",
        "    # Local path to store the raw data\n",
        "    path = \"/content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\"\n",
        "    data, city_pv, tract_pv = get_data(path)\n",
        "    tract_pv.reset_index(level=0, inplace=True)\n",
        "    logging.info(\"Get formatted pivot table data by city and census track from file: \" + path)\n",
        "\n",
        "    if mode == 1:\n",
        "        print(\"You have chose to run top feature selection analysis for health care data\")\n",
        "        top_n = int(input('Enter the number of top features you want to select:\\n'))\n",
        "\n",
        "        logging.info(\"About to run feature selection for city level data\")\n",
        "        feature_selection_df = feature_selection(city_pv, outcome_cols, top_n)\n",
        "        feature_selection_df.to_csv(\"city_data.csv\")\n",
        "        logging.info(\"Country data has been saved to csv\")\n",
        "\n",
        "        logging.info(\"About to run feature selection for each state\")\n",
        "        states = np.unique(tract_pv['StateDesc'])\n",
        "        state_feature_selection_df = pd.DataFrame()\n",
        "        for state in states:\n",
        "            logging.info(\"Feature selection for state %s\" % state)\n",
        "            state_data = tract_pv[tract_pv['StateDesc'] == state]\n",
        "            df = feature_selection(state_data, outcome_cols, top_n)\n",
        "            df[\"state\"] = state\n",
        "            state_feature_selection_df = pd.concat([state_feature_selection_df, df])\n",
        "        state_feature_selection_df.to_csv(\"state_feature_selection.csv\")\n",
        "        logging.info(\"States data has been saved to csv\")\n",
        "    elif mode == 2:\n",
        "        print(\"You have chosen to generate output file for visualization\")\n",
        "\n",
        "        result = pd.DataFrame()\n",
        "        logging.info(\"Start data prep in US level\")\n",
        "        prev_data = data[data['GeographicLevel'] == 'US']\n",
        "\n",
        "        #df = visual_data_prep(city_pv, prev_data, outcome_cols, 5)\n",
        "        df = visual_data_prep(tract_pv, prev_data, outcome_cols, 5)\n",
        "        df['Region'] = 'US'\n",
        "        df['Level'] = 'US'\n",
        "        df['PreventionCalculationLevel'] = 'National'\n",
        "        result = pd.concat([result, df])\n",
        "        logging.info(\"Complete data prep in US level\")\n",
        "\n",
        "        logging.info(\"Start data prep in State level\")\n",
        "        states = np.unique(tract_pv['StateDesc'])\n",
        "        state_df = pd.DataFrame()\n",
        "        for state in states:\n",
        "            state_data = tract_pv[tract_pv['StateDesc'] == state]\n",
        "            logging.info(\"Data prep for state %s with %s census tracks\" % (state, len(state_data)))\n",
        "            prev_data = data[data['GeographicLevel'] == 'City']\n",
        "            prev_data = prev_data[prev_data['StateDesc'] == state]\n",
        "\n",
        "            # for state has more than 100 data points, we conduct another analysis on state level\n",
        "            if len(state_data) > 100:\n",
        "                df = visual_data_prep(state_data, prev_data, outcome_cols, 5)\n",
        "                df['PreventionCalculationLevel'] = 'State'\n",
        "                df[\"Region\"] = state\n",
        "                df['Level'] = 'State'\n",
        "                state_df = pd.concat([state_df, df])\n",
        "\n",
        "            # Append national level data for each state with state prevalence information\n",
        "            df = append_existing_data(result[result.Level == 'US'], prev_data)\n",
        "            df[\"Region\"] = state\n",
        "            df['Level'] = 'State'\n",
        "            df['PreventionCalculationLevel'] = 'National'\n",
        "            state_df = pd.concat([state_df, df])\n",
        "\n",
        "            # For each cities with in the state\n",
        "            cities = np.unique(state_data['CityName'])\n",
        "            national_calc_df = state_df[state_df.PreventionCalculationLevel == 'National']\n",
        "            national_calc_df = national_calc_df[national_calc_df.Region == state]\n",
        "            state_calc_df = state_df[state_df.PreventionCalculationLevel == 'State']\n",
        "            state_calc_df = state_calc_df[state_calc_df.Region == state]\n",
        "\n",
        "            city_df = pd.DataFrame()\n",
        "            for city in cities:\n",
        "                # Apply national level analysis result for each city\n",
        "                city_prev_data = prev_data[prev_data.CityName == city]\n",
        "                df = append_existing_data(national_calc_df, city_prev_data)\n",
        "                df[\"Region\"] = city\n",
        "                df['Level'] = 'City'\n",
        "                city_df = pd.concat([city_df, df])\n",
        "\n",
        "                # Apply state level analysis result for each city if exists\n",
        "                if state_calc_df.shape[0] > 0:\n",
        "                    df = append_existing_data(state_calc_df, city_prev_data)\n",
        "                    df[\"Region\"] = city\n",
        "                    df['Level'] = 'City'\n",
        "                    city_df = pd.concat([city_df, df])\n",
        "            result = pd.concat([result, city_df])\n",
        "        result = pd.concat([result, state_df])\n",
        "        col_order = ['Region','Level','Outcome','Prevention','OutcomePrevalence','PredictionAccuracyScore','PreventionPrevalence','PreventionRank','PreventionCalculationLevel']\n",
        "        result = result [col_order]\n",
        "        logging.info(\"Complete data prep in State level\")\n",
        "\n",
        "        result.to_csv(\"visual_input_tract_with_city.csv\", index=False)\n",
        "        logging.info(\"csv file for visualization has been generated\")\n",
        "    elif mode == 3:\n",
        "        print(\"You have choose to run multiple models comparision analysis.\\n\")\n",
        "        level = input(\"Enter the level of analysis you want to perform (State/US):\\n\")\n",
        "        if level == 'US':\n",
        "            print(\"About to analysis on national level\")\n",
        "            multi_model_analysis(city_pv)\n",
        "        elif level == 'State':\n",
        "            show_state = input(\"Do you wish to see all the state names? (Y/N)\")\n",
        "            if show_state == 'Y':\n",
        "                print( np.unique(tract_pv['StateDesc']) )\n",
        "\n",
        "            state = input(\"Enter the name of the state you want to analyze:\\n\")\n",
        "            state_data = tract_pv[tract_pv['StateDesc'] == state]\n",
        "            if state_data.shape[0] == 0:\n",
        "                print(\"Bad state name, abort!\")\n",
        "                sys.exit()\n",
        "            print(\"About to run model selection for state: \", state, \" with \", len(state_data), \" census track.\")\n",
        "            multi_model_analysis(state_data)\n",
        "        else:\n",
        "            print(\"Unacceptable input, abort!\")\n",
        "\n",
        "\n",
        "main()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "fAhwbHHqwIUD",
        "outputId": "7e79d842-9d1e-41f3-f688-2bc5e5671212"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n",
            "Logger ready\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Accepted mode: \n",
            "1. Perform top feature selection Analysis.\n",
            "2. Generate output file for visualization.\n",
            "3. Run multiple models comparision analysis.\n",
            "\n",
            "Enter the number of your selected mode:\n",
            "3\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "INFO:root:Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n",
            "Get formatted pivot table data by city and census track from file: /content/gdrive/MyDrive/500_Cities__Local_Data_for_Better_Health__2018_release (1).csv\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Size of census tract data: 28004\n",
            "Size of city data: 500\n",
            "You have choose to run multiple models comparision analysis.\n",
            "\n",
            "Enter the level of analysis you want to perform (State/US):\n",
            "US\n",
            "About to analysis on national level\n",
            "-----------------------------------\n",
            "# 0\n",
            "Health Outcome to analyze:  ARTHRITIS\n",
            "lm coef: [-0.04951063 -0.29566126  0.49716485 -0.17651153  0.18102031  0.12261768\n",
            " -0.00839581 -0.11227574  0.62315696  0.09225053 -0.03787054 -0.17382469\n",
            "  0.15760971 -0.01171096  0.02264826]\n",
            "lm train score: 0.8455\n",
            "lm test score: 0.8268\n",
            "-19.662916069664067\n",
            "[-0.04978606 -0.29530243  0.49697395 -0.17619328  0.18093421  0.12258824\n",
            " -0.00839234 -0.11218978  0.62245362  0.09202337 -0.03763863 -0.17398862\n",
            "  0.15778882 -0.01171708  0.0225762 ]\n",
            "Ridge()\n",
            "Ridge train score: 0.8455311571160908\n",
            "Ridge test score: 0.8268958877709699\n",
            "Lasso coef: [-0.12130705 -0.0303183   0.466717   -0.          0.05109257  0.\n",
            " -0.         -0.          0.34877278  0.          0.         -0.10402164\n",
            "  0.14695428 -0.01634848  0.        ]\n",
            "Lasso train score: 0.7957\n",
            "Lasso test score: 0.8041\n"
          ]
        }
      ]
    }
  ]
}